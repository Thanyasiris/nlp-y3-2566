{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Thanyasiris/nlp-y3-2566/blob/main/%5BWeek_2%5D_Library_%26_PythaiNLP.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Install python library\n",
        "PyThaiNLP\n",
        "[https://pythainlp.github.io/dev-docs/](https://pythainlp.github.io/dev-docs/)"
      ],
      "metadata": {
        "id": "wQ1FfNZrNkSJ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YiIbtC22qLKS",
        "outputId": "9f42b1c0-c982-4136-cb74-dfd7acc0a893"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: pythainlp in /usr/local/lib/python3.8/dist-packages (3.1.0)\n",
            "Requirement already satisfied: requests>=2.22.0 in /usr/local/lib/python3.8/dist-packages (from pythainlp) (2.25.1)\n",
            "Requirement already satisfied: chardet<5,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests>=2.22.0->pythainlp) (4.0.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests>=2.22.0->pythainlp) (2022.12.7)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests>=2.22.0->pythainlp) (2.10)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests>=2.22.0->pythainlp) (1.24.3)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting attacut\n",
            "  Downloading attacut-1.0.6-py3-none-any.whl (1.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m39.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: torch>=1.2.0 in /usr/local/lib/python3.8/dist-packages (from attacut) (1.13.0+cu116)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.8/dist-packages (from attacut) (1.15.0)\n",
            "Collecting ssg>=0.0.4\n",
            "  Downloading ssg-0.0.8-py3-none-any.whl (473 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m473.8/473.8 KB\u001b[0m \u001b[31m30.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting docopt>=0.6.2\n",
            "  Downloading docopt-0.6.2.tar.gz (25 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: numpy>=1.17.0 in /usr/local/lib/python3.8/dist-packages (from attacut) (1.21.6)\n",
            "Requirement already satisfied: pyyaml>=5.1.2 in /usr/local/lib/python3.8/dist-packages (from attacut) (6.0)\n",
            "Collecting nptyping>=0.2.0\n",
            "  Downloading nptyping-2.4.1-py3-none-any.whl (36 kB)\n",
            "Collecting fire>=0.1.3\n",
            "  Downloading fire-0.5.0.tar.gz (88 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m88.3/88.3 KB\u001b[0m \u001b[31m7.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: termcolor in /usr/local/lib/python3.8/dist-packages (from fire>=0.1.3->attacut) (2.2.0)\n",
            "Requirement already satisfied: typing-extensions<5.0.0,>=4.0.0 in /usr/local/lib/python3.8/dist-packages (from nptyping>=0.2.0->attacut) (4.4.0)\n",
            "Collecting python-crfsuite>=0.9.6\n",
            "  Downloading python_crfsuite-0.9.8-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m44.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.32.2 in /usr/local/lib/python3.8/dist-packages (from ssg>=0.0.4->attacut) (4.64.1)\n",
            "Building wheels for collected packages: docopt, fire\n",
            "  Building wheel for docopt (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for docopt: filename=docopt-0.6.2-py2.py3-none-any.whl size=13723 sha256=8e4e53294a62fb757dca90a74c2271db4afb953785650eb1c2baacc8e6e404ac\n",
            "  Stored in directory: /root/.cache/pip/wheels/56/ea/58/ead137b087d9e326852a851351d1debf4ada529b6ac0ec4e8c\n",
            "  Building wheel for fire (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for fire: filename=fire-0.5.0-py2.py3-none-any.whl size=116949 sha256=7cf5e7e3a5ba8ea77d3d5fe58e163175da8f7cf5d675c31bb9c5008d0c710ca5\n",
            "  Stored in directory: /root/.cache/pip/wheels/5b/eb/43/7295e71293b218ddfd627f935229bf54af9018add7fbb5aac6\n",
            "Successfully built docopt fire\n",
            "Installing collected packages: python-crfsuite, docopt, nptyping, fire, ssg, attacut\n",
            "Successfully installed attacut-1.0.6 docopt-0.6.2 fire-0.5.0 nptyping-2.4.1 python-crfsuite-0.9.8 ssg-0.0.8\n"
          ]
        }
      ],
      "source": [
        "!pip install pythainlp\n",
        "# !pip install pythainlp==3.1.0\n",
        "# !pip install attacut\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pythainlp.tokenize import word_tokenize\n",
        "\n",
        "text = \"โอเคบ่พวกเรารักภาษาบ้านเกิด\"\n",
        "\n",
        "print(word_tokenize(text, engine=\"newmm\")) #new maximal matching \n",
        "print(word_tokenize(text, engine='longest')) #longest matching \n",
        "\n",
        "print(word_tokenize(text, engine='attacut')) #Neural network\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DLNdP4IdN_FP",
        "outputId": "46dca25d-f1ba-4fe5-81e0-acea5ea92eae"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['โอเค', 'บ่', 'พวกเรา', 'รัก', 'ภาษา', 'บ้านเกิด']\n",
            "['โอเค', 'บ่', 'พวกเรา', 'รัก', 'ภาษา', 'บ้านเกิด']\n",
            "['โอเค', 'บ่', 'พวก', 'เรา', 'รัก', 'ภาษา', 'บ้าน', 'เกิด']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Add parameter"
      ],
      "metadata": {
        "id": "ML7umAeSRS4E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"วรรณกรรม ภาพวาด และการแสดงงิ้ว \"\n",
        "\n",
        "print(word_tokenize(text, engine=\"newmm\"))\n",
        "\n",
        "print(word_tokenize(text, engine=\"newmm\", keep_whitespace=False))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TkU5e6LsQSGx",
        "outputId": "7536766d-c96a-4789-b49f-92bc43f9d364"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['วรรณกรรม', ' ', 'ภาพวาด', ' ', 'และ', 'การแสดง', 'งิ้ว', ' ']\n",
            "['วรรณกรรม', 'ภาพวาด', 'และ', 'การแสดง', 'งิ้ว']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pythainlp.corpus.common import thai_words\n",
        "from pythainlp.util import dict_trie\n",
        "\n",
        "text = 'ชินโซ อาเบะ เกิด 21 กันยายน'\n",
        "\n",
        "print(word_tokenize(text, engine=\"newmm\"))\n",
        "\n",
        "\n",
        "custom_dict_japanese_name = set(thai_words())\n",
        "custom_dict_japanese_name.add('ชินโซ')\n",
        "custom_dict_japanese_name.add('อาเบะ')\n",
        "\n",
        "trie = dict_trie(dict_source=custom_dict_japanese_name)\n",
        "\n",
        "print(word_tokenize(text, engine=\"newmm\", custom_dict=trie))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VxeqqM4KQccK",
        "outputId": "c12b59f1-783d-46c4-dfe6-dcadb06f2414"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['ชิน', 'โซ', ' ', 'อา', 'เบะ', ' ', 'เกิด', ' ', '21', ' ', 'กันยายน']\n",
            "['ชินโซ', ' ', 'อาเบะ', ' ', 'เกิด', ' ', '21', ' ', 'กันยายน']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pythainlp.tokenize.multi_cut import find_all_segment, mmcut, segment\n",
        "\n",
        "find_all_segment(\"มีความเป็นไปได้อย่างไรบ้าง\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y_9W-nHPRPzR",
        "outputId": "1fed0353-5e62-445c-8b50-e3b6291031e0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['มี|ความ|เป็น|ไป|ได้|อย่าง|ไร|บ้าง|',\n",
              " 'มี|ความ|เป็นไป|ได้|อย่าง|ไร|บ้าง|',\n",
              " 'มี|ความ|เป็นไปได้|อย่าง|ไร|บ้าง|',\n",
              " 'มี|ความเป็นไป|ได้|อย่าง|ไร|บ้าง|',\n",
              " 'มี|ความเป็นไปได้|อย่าง|ไร|บ้าง|',\n",
              " 'มี|ความ|เป็น|ไป|ได้|อย่างไร|บ้าง|',\n",
              " 'มี|ความ|เป็นไป|ได้|อย่างไร|บ้าง|',\n",
              " 'มี|ความ|เป็นไปได้|อย่างไร|บ้าง|',\n",
              " 'มี|ความเป็นไป|ได้|อย่างไร|บ้าง|',\n",
              " 'มี|ความเป็นไปได้|อย่างไร|บ้าง|',\n",
              " 'มี|ความ|เป็น|ไป|ได้|อย่างไรบ้าง|',\n",
              " 'มี|ความ|เป็นไป|ได้|อย่างไรบ้าง|',\n",
              " 'มี|ความ|เป็นไปได้|อย่างไรบ้าง|',\n",
              " 'มี|ความเป็นไป|ได้|อย่างไรบ้าง|',\n",
              " 'มี|ความเป็นไปได้|อย่างไรบ้าง|']"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pythainlp.util import normalize\n",
        "normalize(\"เเปลก\") \n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 37
        },
        "id": "eL7oleK9SJaM",
        "outputId": "8ff2b181-7ac2-4fea-e867-5107d4940624"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'แปลก'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pythainlp as thnlp\n",
        "\n",
        "thnlp.util.normalize(\" เเปลก\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 37
        },
        "id": "5Yr8j6-fSN5J",
        "outputId": "655f5115-d585-49a8-b11c-569a7c0ac9a8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'แปลก'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pythainlp import word_tokenize\n",
        "\n",
        "text = \"เก็บวันน้ี พรุ่งน้ีก็เกา่\"\n",
        "\n",
        "print(\"tokenize immediately:\")\n",
        "print(word_tokenize(text))\n",
        "print(\"\\nnormalize, then tokenize:\")\n",
        "print(word_tokenize(normalize(text)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7-4pbWocSqU-",
        "outputId": "bcadf54f-8163-4087-f69e-e5d2ccaa7f9d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tokenize immediately:\n",
            "['เก็บ', 'วัน', 'น้ี', ' ', 'พรุ่งน้ี', 'ก็', 'เกา', '่']\n",
            "\n",
            "normalize, then tokenize:\n",
            "['เก็บ', 'วันนี้', ' ', 'พรุ่งนี้', 'ก็', 'เก่า']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "normalize(\"เกะะะ\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 37
        },
        "id": "I_jL_JiaSzEZ",
        "outputId": "7f1c1d38-fb4e-4bd0-e7a4-a25d1b2640d8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'เกะ'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Internally, `normalize()` is just a series of function calls like this:\n",
        "```\n",
        "text = remove_zw(text)\n",
        "text = remove_dup_spaces(text)\n",
        "text = remove_repeat_vowels(text)\n",
        "text = remove_dangling(text)\n",
        "```\n",
        "\n",
        "If you don't like the behavior of default `normalize()`, you can call those functions shown above, also `remove_tonemark()` and `reorder_vowels()`, individually from `pythainlp.util`, to customize your own normalization.\n",
        "\n",
        "[https://pythainlp.github.io/docs/3.1/notes/FAQ.html](https://pythainlp.github.io/docs/3.1/notes/FAQ.html)"
      ],
      "metadata": {
        "id": "nTI_rrvwS0Lh"
      }
    }
  ]
}